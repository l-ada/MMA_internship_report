\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

% Title Page
\title{Investigation of implicit approach to loss function}
\author{Åukasz Adamowicz}

\usepackage{mathcommands}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\begin{document}
\maketitle

\begin{abstract}
TK make sure every equation is numbered
\end{abstract}



\section{ Lab presentation}
SciAi groups research focus is on ...

TK
We assume $E(\theta, p)$ is ??? differentiable/continuous

Electron density is approximated by a set of finite-dimensional  atom-centered function
\begin{equation}
 \rho(r) = \sum_{i=1}^n p_i \omega_i(r)
\end{equation}


Representation of electron density
 \section{Math part}
Notation: Let $E(\theta, M ,p)$ denote the energy model, with $\theta$ being model parameters, $M$ the molecule and $p=(p_1, \ldots ,p_n )\in \R^n$ density coefficients. Vector $w$ corresponds to
TK $p$ DIMENSION DEPENDS ON N $M$
TK WHAT IS THE DIMENSION OF p?

 \subsection{Implicit}
1. Implicit funciton theorem

The goal of the project was to investigate and minimize the following loss
\begin{equation}
 \min_\theta \sum_i \mathcal{L}(p_{\theta}^{M_i}),
\end{equation}
where
\begin{equation}
  p_\theta^{M_i}:=\argmin_{p : \inner{w}{p}=N_i} E(\theta,M_i,p)
\end{equation}
and $\mathcal{L}_{M_i}(p) = \frac{1}{2}\mse{p-p_{M_i}}$ is the standard $L_2$ loss.

In the rest of the report I will drop index $M_i$, since it the loss is a sum of individual losses for each molecule and it will notably simplify the notation.
Therefore I will write $E(\theta,p)$ instead of $E(\theta,M,p)$ and the same for $p_\theta$, $\mathcal{L}(p)$ and so on.

This falls under the domain of \textbf{bilevel optimization}.

The main problem lies in computing the gradient of $\mathcal{L}(p_\theta)$


\subsubsection{IFT for one molecule}
Let $w$ be a non-zero vector and $N>0$ a positive number.
We consider the constrained problem of minimizing

\begin{equation}
 \min_\theta \mathcal{L}(p_\theta),
\end{equation}
where
\begin{equation}
\argmin_{\substack{p \in \R^n \\ p: \inner{p}{w}=N }}
\end{equation}
and
\begin{equation}
 \mathcal{L}(p) = \frac{1}{2} \norm{p-p_{gs}}^2,
\end{equation}
where $p_{gs}$ are ground state density coefficiets given from the data and satisfy the constraint $\inner{p_{gs}}{w} = N$.

This can be accomplished by using implicit function theorem

TK ADD STATEMENT
\begin{theorem}[Implicit Function Theorem]
Your theorem statement.
\end{theorem}

\subsubsection{Basis dependent approach}
TK find better notation

First, let's consider the problem in an orthogonal basis containing $w$.
Let the basis be $(e_1,\ldots, e_{n-1}, w)$.
Let $\widetilde{E}(\theta, p) = E(\theta, p_{gs}+ p)$.



For simplicity we drop the
% General formula
\begin{equation}
 \dth{\mathcal{L}(\pth)} = \pd{\mathcal{L}}{p}\dth{\pth}.
\end{equation}
Since
\begin{equation}
 \pth = \argmin_p E(\theta, p),
\end{equation}
it follows that
\begin{equation}
 \nabla_p E(\theta, \pth) = 0
\end{equation}
and by taking derivative wrt. $\theta$ we get


% total derivative
\begin{equation}
0=\dth{E(\theta, p_\theta)} = \pd{E}{\theta} + \pd{E}{p} \dth{\pth}.
\end{equation}
Therefore
\begin{equation}
 \dth{\pth} = -A^{-1} \pd{\nabla_p E}{\theta}(\theta, \pth),
\end{equation}
where

\begin{equation}
 A := \frac{\partial E(\theta, p)}{\partial p}\bigg|_{p=p_\theta, \theta=\theta}
\end{equation}


\begin{equation}
 \dth{\mathcal{L}(\pth)} = - (\pth - p_{gs}) A^{-1}  \frac{\partial \nabla_p E}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}


Then following \cite{neuralscf} we denote
\begin{equation}
 y = -(\pth-p_{gs})A^{-1}.
\end{equation}
The final gradient formula is
\begin{equation}
 \dth{\mathcal{L}(\pth)} = y \cdot \frac{\partial \nabla_p E}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}


The quantity $\frac{\partial \nabla_p E}{\partial \theta}$ is obtained using automatic differentation and $y$ is a solution to linear system $yA = -(\pth-p_{gs})$. Solving for $y$ is done using matrix-free methods such as conjugate gradient method or plain gradient descent on square error loss $\mse{yA + \pth+p_{gs}}$ since $yA$ is vector-jacobian product, which also can be evaluated efficiently using automatic differentation. Since $E$ is ..., and a approximation of physical potential function, it's reasonable to assume that it is positive-definite.

% END OF basis approach
\subsubsection{Basis-free approach}
% Gradient formula
\begin{statement}
The gradient of $\mathcal{L}(\pth)$ is equal to
 \begin{equation}
 \dth{\mathcal{L}(\pth)} = -(\pth - p_{gs}) \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}
\end{equation}
\end{statement}


TK DISCLAIMERS ABOUT APPLICABILITY, IT BEING A FUNCTION ETC, JACOBIAN

TK DOES IT EVEN WORK IF IT'S NOT A FUNCTION
\subsubsection{Derivation of the gradient}
To obtain the gradient of the loss we start from chain rule
\begin{equation}
 \dth{\mathcal{L}(\pth)} = \pd{\mathcal{L}}{p}\dth{\pth}.
\end{equation}
Since $\pd{\mathcal{L}}{p}(\pth )= \pth-p_{gs}$ all we need to find is $\dth{\pth}.$ To this end we make use of the following lemma.

\begin{lemma}
 Let $P := Id -\frac{ww^T}{w^Tw} $ be the projection operator onto subspace $V = \text{span}(w)^{\perp}$. Then the following equality is true
 \begin{equation}
  P\big(\nabla_p E(\theta, p_\theta)\big) = 0
 \end{equation}

\end{lemma}

\begin{proof}
  Since $p_\theta$ is the minimum of $E(\theta,p)$ restricted to $V$ we have \[\langle \nabla_p E(\theta,p_{\theta}), u \rangle = 0\] for any $u\in V$. Taking $u = P\big( \nabla_p E(\theta,p_\theta)\big)$ we obtain
  \[\langle \nabla_p E(\theta,p_{\theta}), P\big( \nabla_p E(\theta,p_\theta)\big) \rangle = \norm{P\big( \nabla_p E(\theta,p_\theta)\big)}^2=0,\]
  which conludes the proof.
\end{proof}

Let's denote $F(\theta,p) :=P (\nabla_p E(\theta,p)).$ Then the previous condition can be written as
\begin{equation}
 F(\theta,p_\theta) = 0.
\end{equation}
We take the derivative with respect to $\theta$
\begin{equation}
0=\dth{F(\theta, p_\theta)} = \pd{F}{\theta} + \pd{F}{p} \dth{\pth}.
\end{equation}
Rearranging we obtain
\begin{equation}
 \frac{d p_\theta}{d\theta} = - \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}

Therefore the full gradient of $\mathcal{L}(\pth)$ is equal to
\begin{equation}
 \dth{\mathcal{L}(\pth)} = - (\pth - p_{gs})\bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}
\begin{remark}
 The jacobian $\frac{\partial F}{\partial p}$ is not invertible, since $F$ is a projection.
 \begin{equation}
  F(\theta, p) = \sum_{i=1}^{n-1} \inner{\nabla_p E(\theta, p)}{e_i} e_i +  0 \cdot w
 \end{equation}

 The jacobian matrix in the basis $(e_0,...,e_{n-1},w)$ can be written as \begin{equation}
                      \frac{\partial F}{\partial p}\bigg|_{p=\pth} =\begin{bmatrix}
A & 0 \\
0 & 0
\end{bmatrix}.
                     \end{equation}



 $\pth = \sum_{i=1}^{n-1}p_i(\theta) e_i +  0 \cdot w$
\end{remark}

The solution to linear regression
\begin{equation}
 y = \underset{u\in \R^n}{\mathrm{argmin }}\bigg\|u \begin{bmatrix}
A & 0 \\
0 & 0
\end{bmatrix} + (p_\theta-p_{true})\bigg\|^2
\end{equation}
is satisfied for any $y$ of the form
\begin{equation}
 y = P\big((\pth-p_{gs})\big)A^{-1} + \alpha w.
\end{equation}

To make $y$ unique we add a term such as $\inner{u}{w}^2$ or add additional constraints.

TK w TERM SHOULD VANISH AFTER

$$ \frac{d}{d\theta} \nabla_p E(\theta,p_\theta) = 0.$$
 Let's denote $F(\theta,p) = \nabla_p E(\theta,p)$ and let $J = J_{F}(p_\theta)=\frac{\partial F}{\partial p}$ be the jacobian of $F$ at $p = p_\theta$.
We obtain

$$\frac{d}{d\theta}F(\theta,p_\theta) = \frac{\partial F}{\partial \theta} + \frac{\partial F}{\partial p}\frac{d p_\theta}{d\theta}= 0 .$$



3. Lagrangian approach?

TK COME BACK WHEN YOU ACTUALLY DO IT BY HAND
 \begin{equation}
\min_{\lambda,\theta} L
 \end{equation}

4. Proof of IFT with constraints converging

5. Error analysis for both approximate fixed point and solving linear equation
TK add section from Sacramento,Zuccheti
\subsection{ Equilibrium propagation}

\subsubsection{Base version}

Let's denote
\begin{equation}
H(\theta, p, \beta) := \beta \mathcal{L}(p) + E(\theta, p),
\end{equation}
where $\beta \in \R$.
 This is called \textbf{total energy} in \cite{eqprop}. Let's also denote
\begin{equation}
p_{\theta}^{\beta}:=\argmin_p H(\theta, p, \beta)
\end{equation}


Since $H(\theta,p,0) = E(\theta,p)$ we have $p_{\theta}^{0}=p_{\theta}$.

\begin{statement}[Equilibrium propagation formula]
 \begin{equation}
 \frac{d}{d\theta} \mathcal{L}(p_\theta) = \lim_{\beta \to 0} \frac{\pd{H}{\theta}(\theta, p_\theta^\beta, \beta)-\pd{H}{\theta}(\theta, p_\theta^0, 0) }{\beta} = \lim_{\beta \to 0}\frac{1}{\beta} \pd{}{\theta} \bigg(H(\theta, p_\theta^\beta,\beta) - H(\theta, p_\theta^0, 0)\bigg).
\end{equation}
\end{statement}
This formula can be used to numerically approximate the gradient of the loss function.
Since $\mathcal{L}(p)$ doesn't depend on $\theta$, $\pd{H}{\theta}$ simplifies to $\pd{E}{\theta.}$

3. EqProp with constraints



4. Proof of convergence

5. Error margins
\section{ Implementation}
0. Technical details
    architecture
    training
    etc
1. Stability techniques

2. algorithm
\section{Results}
\subsection{Implicit function approach}
\subsubsection{assuming symmetric second derivative}
\subsubsection{one molecule}
\subsubsection{Convergence thresholds}

\nocite{*}
\bibliographystyle{plain}
\bibliography{references.bib}


\section{Appendices}
\appendix
\section{Proof of Equilibrium Propagation formula}
and
\begin{equation}
 G(\theta, \beta) := H(\theta, p_\theta^\beta, \beta).
\end{equation}

\begin{equation}
 \frac{d}{d\theta}\frac{d}{d\beta}G(\theta,\beta)\big|_{\beta=0} = \frac{d}{d\theta} \mathcal{L}(p_\theta)
\end{equation}

\end{document}

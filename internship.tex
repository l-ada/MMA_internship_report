\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

% Title Page
\author{Łukasz Adamowicz}

\usepackage{mathcommands}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\renewcommand{\thesection}{\arabic{section}}
\begin{document}
% \maketitle
\begin{titlepage}
    \begin{center}
        \textbf{\Large Title of Your Internship}\\[2cm]
        \textbf{Academic Year:} 2024--2025\\
        \textbf{Master's Program:} Mathématiques, Modélisation et Apprentissage\\
        \textbf{Name:} Ada Lovelace\\[1cm]
        \textbf{Internship Supervisor:} Alan Turing\\
        \textbf{Email:} alan.turing@company.com\\[1cm]
        \textbf{Company/Host Laboratory:}\\
        Heidelberg University\\
        Interdisciplinary Center for Scientific Computing\\
        Hamprecht Lab\\[1cm]
        \textbf{UFR Contact:}\\
        Grace Hopper\\
        grace.hopper@university.edu\\[2cm]

        % Optionally add logos, date, etc.
        \vfill
        \today
    \end{center}
\end{titlepage}


\begin{abstract}
TK make sure every equation is numbered
During my internship at Hamprecht Lab I investigated training deep learning models using loss function that's defined in an implicit way. I tried two approaches,
\end{abstract}


\tableofcontents

\section{Lab presentation}
SciAi groups research focus is on ...
The goal is to approximate energy functional...
This would allow for computation of ... in complexity O(n) instead of ...
TK
We assume $E(\theta, p)$ is ??? differentiable/continuous

Electron density is approximated by a set of finite-dimensional  atom-centered function


\begin{equation}
 \rho(r) = \sum_{i=1}^n p_i \omega_i(r)
\end{equation}


Representation of electron density
\section{Carried out work}
Notation: Let $E(\theta, M ,p)$ denote the energy model, with $\theta$ being model parameters, $M$ the molecule and $p=(p_1, \ldots ,p_n )\in \R^n$ density coefficients. Vector $w$ corresponds to
TK $p$ DIMENSION DEPENDS ON N $M$
TK WHAT IS THE DIMENSION OF p?

 \subsection{Implicit}
1. Implicit funciton theorem

The goal of the project was to investigate and minimize the following loss
\begin{equation}
 \min_\theta \sum_i \mathcal{L}(p_{\theta}^{M_i}),
\end{equation}
where
\begin{equation}
  p_\theta^{M_i}:=\argmin_{p : \inner{w}{p}=N_i} E(\theta,M_i,p)
\end{equation}
and $\mathcal{L}_{M_i}(p) = \frac{1}{2}\mse{p-p_{M_i}}$ is the standard $L_2$ loss.

In the rest of the report I will drop index $M_i$, since it the loss is a sum of individual losses for each molecule and it will notably simplify the notation.
Therefore I will write $E(\theta,p)$ instead of $E(\theta,M,p)$ and the same for $p_\theta$, $\mathcal{L}(p)$ and so on.

This falls under the domain of \textbf{bilevel optimization}.

The main problem lies in computing the gradient of $\mathcal{L}(p_\theta)$


\subsubsection{IFT for one molecule}
Let $w$ be a non-zero vector and $N>0$ a positive number.
We consider the constrained problem of minimizing

\begin{equation}
 \min_\theta \mathcal{L}(p_\theta),
\end{equation}
where
\begin{equation}
\argmin_{\substack{p \in \R^n \\ p: \inner{p}{w}=N }}
\end{equation}
and
\begin{equation}
 \mathcal{L}(p) = \frac{1}{2} \norm{p-p_{gs}}^2,
\end{equation}
where $p_{gs}$ are ground state density coefficiets given from the data and satisfy the constraint $\inner{p_{gs}}{w} = N$.

This can be accomplished by using implicit function theorem

TK ADD STATEMENT
\begin{theorem}[Implicit Function Theorem]
Your theorem statement.
\end{theorem}

\subsubsection{Basis dependent approach}
TK find better notation

First, let's consider the problem in an orthogonal basis containing $w$.
Let the basis be $(e_1,\ldots, e_{n-1}, w)$.
Let $\widetilde{E}(\theta, p) = E(\theta, p_{gs}+ p)$.



For simplicity we drop the
% General formula
\begin{equation}
 \dth{\mathcal{L}(\pth)} = \pd{\mathcal{L}}{p}\dth{\pth}.
\end{equation}
Since
\begin{equation}
 \pth = \argmin_p E(\theta, p),
\end{equation}
it follows that
\begin{equation}
 \nabla_p E(\theta, \pth) = 0
\end{equation}
and by taking derivative wrt. $\theta$ we get


% total derivative
\begin{equation}
0=\dth{E(\theta, p_\theta)} = \pd{E}{\theta} + \pd{E}{p} \dth{\pth}.
\end{equation}
Therefore
\begin{equation}
 \dth{\pth} = -A^{-1} \pd{\nabla_p E}{\theta}(\theta, \pth),
\end{equation}
where

\begin{equation}
 A := \frac{\partial E(\theta, p)}{\partial p}\bigg|_{p=p_\theta, \theta=\theta}
\end{equation}


\begin{equation}
 \dth{\mathcal{L}(\pth)} = - (\pth - p_{gs}) A^{-1}  \frac{\partial \nabla_p E}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}


Then following \cite{neuralscf} we denote
\begin{equation}
 y = -(\pth-p_{gs})A^{-1}.
\end{equation}
The final gradient formula is
\begin{equation}
 \dth{\mathcal{L}(\pth)} = y \cdot \frac{\partial \nabla_p E}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}


The quantity $\frac{\partial \nabla_p E}{\partial \theta}$ is obtained using automatic differentation and $y$ is a solution to linear system $yA = -(\pth-p_{gs})$. Solving for $y$ is done using matrix-free methods such as conjugate gradient method or plain gradient descent on square error loss $\mse{yA + \pth+p_{gs}}$ since $yA$ is vector-jacobian product, which also can be evaluated efficiently using automatic differentation. Since $E$ is ..., and a approximation of physical potential function, it's reasonable to assume that it is positive-definite.

% END OF basis approach
\subsubsection{Basis-free approach}
% Gradient formula
\begin{statement}
The gradient of $\mathcal{L}(\pth)$ is equal to
 \begin{equation}
 \dth{\mathcal{L}(\pth)} = -(\pth - p_{gs}) \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}
\end{equation}
\end{statement}


TK DISCLAIMERS ABOUT APPLICABILITY, IT BEING A FUNCTION ETC, JACOBIAN

TK DOES IT EVEN WORK IF IT'S NOT A FUNCTION
\subsubsection{Derivation of the gradient}
To obtain the gradient of the loss we start from chain rule
\begin{equation}
 \dth{\mathcal{L}(\pth)} = \pd{\mathcal{L}}{p}\dth{\pth}.
\end{equation}
Since $\pd{\mathcal{L}}{p}(\pth )= \pth-p_{gs}$ all we need to find is $\dth{\pth}.$ To this end we make use of the following lemma.

\begin{lemma}
 Let $P := Id -\frac{ww^T}{w^Tw} $ be the projection operator onto subspace $V = \text{span}(w)^{\perp}$. Then the following equality is true
 \begin{equation}
  P\big(\nabla_p E(\theta, p_\theta)\big) = 0
 \end{equation}

\end{lemma}

\begin{proof}
  Since $p_\theta$ is the minimum of $E(\theta,p)$ restricted to $V$ we have \[\langle \nabla_p E(\theta,p_{\theta}), u \rangle = 0\] for any $u\in V$. Taking $u = P\big( \nabla_p E(\theta,p_\theta)\big)$ we obtain
  \[\langle \nabla_p E(\theta,p_{\theta}), P\big( \nabla_p E(\theta,p_\theta)\big) \rangle = \norm{P\big( \nabla_p E(\theta,p_\theta)\big)}^2=0,\]
  which conludes the proof.
\end{proof}

Let's denote $F(\theta,p) :=P (\nabla_p E(\theta,p)).$ Then the previous condition can be written as
\begin{equation}
 F(\theta,p_\theta) = 0.
\end{equation}
We take the derivative with respect to $\theta$
\begin{equation}
0=\dth{F(\theta, p_\theta)} = \pd{F}{\theta} + \pd{F}{p} \dth{\pth}.
\end{equation}
Rearranging we obtain
\begin{equation}
 \frac{d p_\theta}{d\theta} = - \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}

Therefore the full gradient of $\mathcal{L}(\pth)$ is equal to
\begin{equation}
 \dth{\mathcal{L}(\pth)} = - (\pth - p_{gs})\bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}
\begin{remark}
 The jacobian $\frac{\partial F}{\partial p}$ is not invertible, since $F$ is a projection.
 \begin{equation}
  F(\theta, p) = \sum_{i=1}^{n-1} \inner{\nabla_p E(\theta, p)}{e_i} e_i +  0 \cdot w
 \end{equation}

 The jacobian matrix in the basis $(e_0,...,e_{n-1},w)$ can be written as \begin{equation}
                      \frac{\partial F}{\partial p}\bigg|_{p=\pth} =\begin{bmatrix}
A & 0 \\
0 & 0
\end{bmatrix}.
                     \end{equation}



 $\pth = \sum_{i=1}^{n-1}p_i(\theta) e_i +  0 \cdot w$
\end{remark}

The solution to linear regression
\begin{equation}
 y = \underset{u\in \R^n}{\mathrm{argmin }}\bigg\|u \begin{bmatrix}
A & 0 \\
0 & 0
\end{bmatrix} + (p_\theta-p_{true})\bigg\|^2
\end{equation}
is satisfied for by $y$ of the form

TK RESOLVE DIMENSIONALITY NOTATION PROBLEMS. IS $P:R^n ->R^n-1$ or does is just put zero on the last component?
\begin{equation}
 y = P\big((\pth-p_{gs})\big)A^{-1} + \alpha w.
\end{equation}
Since $F$ is a projection onto plane perpendicular to $w$, it shouldn't matter which $y$ we choose.
That is to say
\begin{equation}
 \inner{y}{ \pd{F}{\theta}}=\inner{Py}{\pd{F}{\theta}}.
\end{equation}

% To make $y$ unique we add a term such as $\inner{u}{w}^2$ or add additional constraints.

algorithm


\begin{algorithm}
\caption{Loss gradient computation}
\begin{algorithmic}
\Require Dataset $\{x_0^{(i)}\}$, noise schedule $\{\beta_t\}$
\Repeat
    \State $x_0 \sim q(x_0)$
    \State $t \sim \text{Uniform}(\{1, \ldots, T\})$
    \State $\epsilon \sim \mathcal{N}(0, I)$
    \State Take gradient step on $\nabla_\theta \|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\|^2$
\Until{converged}
\end{algorithmic}
\end{algorithm}


3. Lagrangian approach?

TK COME BACK WHEN YOU ACTUALLY DO IT BY HAND
 \begin{equation}
\min_{\lambda,\theta} L
 \end{equation}

4. Proof of IFT with constraints converging

5. Error analysis for both approximate fixed point and solving linear equation
TK add section from Sacramento,Zuccheti
\subsection{ Equilibrium propagation}

\subsubsection{Base version}
During my internship I also implemented and tested optimization using Equilibrium Propagation (see \cite{eqprop}, \cite{zucchet2022beyond}).
Let's denote
\begin{equation}
H(\theta, p, \beta) := \beta \mathcal{L}(p) + E(\theta, p),
\end{equation}
where $\beta \in \R$.
 This is called \textbf{total energy} in \cite{eqprop}. Let's also denote
\begin{equation}
p_{\theta}^{\beta}:=\argmin_p H(\theta, p, \beta)
\end{equation}


Since $H(\theta,p,0) = E(\theta,p)$ we have $p_{\theta}^{0}=p_{\theta}$.

\begin{statement}[Equilibrium propagation formula]
 \begin{equation}
 \frac{d}{d\theta} \mathcal{L}(p_\theta) = \lim_{\beta \to 0} \frac{\pd{H}{\theta}(\theta, p_\theta^\beta, \beta)-\pd{H}{\theta}(\theta, p_\theta^0, 0) }{\beta} = \lim_{\beta \to 0}\frac{1}{\beta} \pd{}{\theta} \bigg(H(\theta, p_\theta^\beta,\beta) - H(\theta, p_\theta^0, 0)\bigg).
\end{equation}
\end{statement}
This formula can be used to numerically approximate the gradient of the loss function.
Since $\mathcal{L}(p)$ doesn't depend on $\theta$, $\pd{H}{\theta}$ simplifies to $\pd{E}{\theta.}$

3. EqProp with constraints



4. Proof of convergence

5. Error margins
\subsection{ Implementation}
0. Technical details
    architecture
    training
    etc
1. Stability techniques

2. algorithm
\subsection{Results}
\subsubsection{Implicit function approach}
\subsubsection{assuming symmetric second derivative}
\subsubsection{one molecule}
\subsubsection{Convergence thresholds}
\section{Conclusions}

\nocite{*}
\bibliographystyle{plain}
\bibliography{references.bib}


\section{Appendices}
\appendix
\section{Proof of Equilibrium Propagation formula}
and
\begin{equation}
 G(\theta, \beta) := H(\theta, p_\theta^\beta, \beta).
\end{equation}

\begin{equation}
 \frac{d}{d\theta}\frac{d}{d\beta}G(\theta,\beta)\big|_{\beta=0} = \frac{d}{d\theta} \mathcal{L}(p_\theta)
\end{equation}
\appendix
% \appendixname{Proof of}
\section{Proof of implicit function theorem}

\end{document}

\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

% Title Page
\title{Investigation of implicit approach to loss function}
\author{Åukasz Adamowicz}

\usepackage{mathcommands}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\begin{document}
\maketitle

\begin{abstract}
TK make sure every equation is numbered
\end{abstract}



\section{ Lab presentation}
SciAi groups research focus is on ...

TK
We assume $E(\theta, p)$ is ??? differentiable/continuous

Representation of electron density
 \section{Math part}
Notation: Let $E(\theta, M ,p)$ denote the energy model, with $\theta$ being model parameters, $M$ the molecule and $p\in \R^n$ density coefficients.
TK $p$ DIMENSION DEPENDS ON N $M$

 \subsection{Implicit}
1. Implicit funciton theorem

The goal of the project was to investigate and minimize the following loss
\begin{equation}
 \min_\theta \sum_i \mathcal{L}(p_{\theta}^{M_i}),
\end{equation}
where
\begin{equation}
  p_\theta^{M_i}:=\argmin_{p : \inner{w}{p}=N_i} E(\theta,M_i,p)
\end{equation}
and $\mathcal{L}_{M_i}(p) = \frac{1}{2}\mse{p-p_{M_i}}$ is the standard $L_2$ loss.

In the rest of the report I will drop index $M_i$, since it the loss is a sum of individual losses for each molecule and it will notably simplify the notation.
Therefore I will write $E(\theta,p)$ instead of $E(\theta,M,p)$ and the same for $p_\theta$, $\mathcal{L}(p)$ and so on.

This falls under the domain of \textbf{bilevel optimization}.

The main problem lies in computing the gradient of $\mathcal{L}(p_\theta)$


\subsubsection{ IFT for one molecule}
Let $w$ be a non-zero vector and $N>0$ a positive number.
We consider the constrained problem of minimizing

\begin{equation}
 \min_\theta \mathcal{L}(p_\theta),
\end{equation}
where
\begin{equation}
 p_\theta = \argmin_{p: \inner{p}{w}=N} E(\theta, p)
\end{equation}
and
\begin{equation}
 \mathcal{L}(p) = \frac{1}{2} \norm{p-p_{gs}}^2,
\end{equation}
where $p_{gs}$ are ground state density coefficiets given from the data.

This can be accomplished by using implicit function theorem

TK ADD STATEMENT
\begin{theorem}[Implicit Function Theorem]
Your theorem statement.
\end{theorem}

\begin{statement}
The gradient of $\mathcal{L}(\pth)$ is equal to
 \begin{equation}
 \dth{\mathcal{L}(\pth)} = -(\pth - p_{gs}) \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}
\end{equation}
\end{statement}


TK DISCLAIMERS ABOUT APPLICABILITY, IT BEING A FUNCTION ETC, JACOBIAN

TK DOES IT EVEN WORK IF IT'S NOT A FUNCTION
\subsubsection{Derivation of the gradient}
To obtain the gradient of the loss we start from chain rule
\begin{equation}
 \dth{\mathcal{L}(\pth)} = \pd{\mathcal{L}}{p}\dth{\pth}.
\end{equation}
Since $\pd{\mathcal{L}}{p}(\pth )= \pth-p_{gs}$ all we need to find is $\dth{\pth}.$ To this end we make use of the following lemma.

\begin{lemma}
 Let $P := Id -\frac{ww^T}{w^Tw} $ be the projection operator onto subspace $V = \text{span}(w)^{\perp}$. Then the following equality is true
 \begin{equation}
  P\big(\nabla_p E(\theta, p_\theta)\big) = 0
 \end{equation}

\end{lemma}

\begin{proof}
  Since $p_\theta$ is the minimum of $E(\theta,p)$ restricted to $V$ we have \[\langle \nabla_p E(\theta,p_{\theta}), u \rangle = 0\] for any $u\in V$. Taking $u = P\big( \nabla_p E(\theta,p_\theta)\big)$ we obtain
  \[\langle \nabla_p E(\theta,p_{\theta}), P\big( \nabla_p E(\theta,p_\theta)\big) \rangle = \norm{P\big( \nabla_p E(\theta,p_\theta)\big)}^2=0,\]
  which conludes the proof.
\end{proof}

Let's denote $F(\theta,p) :=P (\nabla_p E(\theta,p)).$ Then the previous condition can be written as
\begin{equation}
 F(\theta,p_\theta) = 0.
\end{equation}
We take the derivative with respect to $\theta$
\begin{equation}
0=\dth{F(\theta, p_\theta)} = \pd{F}{\theta} + \pd{F}{p} \dth{\pth}.
\end{equation}
Rearranging we obtain
\begin{equation}
 \frac{d p_\theta}{d\theta} = - \bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}

Therefore the full gradient of $\mathcal{L}(\pth)$ is equal to
\begin{equation}
 \dth{\mathcal{L}(\pth)} = - (\pth - p_{gs})\bigg(\frac{\partial F}{\partial p}\bigg)^{-1}  \frac{\partial F}{\partial \theta}\bigg|_{p=p_\theta, \theta=\theta}.
\end{equation}
\begin{remark}
 The jacobian $\frac{\partial F}{\partial p}$ is not invertible, since $F$ is a projection.
 \begin{equation}
  F(\theta, p) = \sum_{i=1}^{n-1} \inner{\nabla_p E(\theta, p)}{e_i} e_i +  0 \cdot w
 \end{equation}

 The jacobian matrix in the basis $(e_0,...,e_{n-1},w)$ can be written as \begin{equation}
                      \frac{\partial F}{\partial p} =\begin{bmatrix}
A & 0 \\
0 & 0
\end{bmatrix}
                     \end{equation}

 $\pth = \sum_{i=1}^{n-1}p_i(\theta) e_i +  0 \cdot w$
\end{remark}

as a minimal square problem
$$y = \underset{u}{\mathrm{argmin }}\|uJ + 2(p_\theta-p_{true})\|^2$$ and adjust accordingly, while computing it.


$$ \frac{d}{d\theta} \nabla_p E(\theta,p_\theta) = 0.$$
 Let's denote $F(\theta,p) = \nabla_p E(\theta,p)$ and let $J = J_{F}(p_\theta)=\frac{\partial F}{\partial p}$ be the jacobian of $F$ at $p = p_\theta$.
We obtain

$$\frac{d}{d\theta}F(\theta,p_\theta) = \frac{\partial F}{\partial \theta} + \frac{\partial F}{\partial p}\frac{d p_\theta}{d\theta}= 0 .$$

**Remark** : We treat $p_\theta$ as a function of $\theta$, which may not be applicable for $E$.
For example, there might be multiple minima.

In the end, we obtain

$$ \frac{d p_\theta}{d\theta} = - J^{-1} \frac{\partial F}{\partial \theta}.$$

Thus,

$$\frac{d J}{d\theta} = 2*(p_\theta-p_{true})\cdot \frac{d p_\theta}{d\theta} = 2*(p_\theta-p_{true})\cdot (-J^{-1}) \frac{\partial F}{\partial \theta}.$$

The usual way to deal with this equation is to solve

$$y =- 2*(p_\theta-p_{true}) \cdot J^{-1}$$
or equivalently
$$ yJ = -2*(p_\theta - p_{true}),$$ which is just a linear system of equations.

3. Lagrangian approach?

TK COME BACK WHEN YOU ACTUALLY DO IT BY HAND
 \begin{equation}
\min_{\lambda,\theta} L
 \end{equation}

4. Proof of IFT with constraints converging

5. Error analysis for both approximate fixed point and solving lineare equation
TK add section from Sacramento,Zuccheti
\subsection{ Equilibrium propagation}

\subsubsection{Base version}

Let's denote
\begin{equation}
H(\theta, p, \beta) := \beta \mathcal{L}(p) + E(\theta, p),
\end{equation}
where $\beta \in \R$.
 This is called \textbf{total energy} in \cite{eqprop}. Let's also denote
\begin{equation}
p_{\theta}^{\beta}:=\argmin_p H(\theta, p, \beta)
\end{equation}
and
\begin{equation}
 G(\theta, \beta) = H(\theta, p_\theta^\beta, \beta).
\end{equation}

Since $H(\theta,p,0) = E(\theta,p)$ we have $p_{\theta}^{0}=p_{\theta}$.

Statement:

\begin{equation}
 \frac{d}{d\theta} \mathcal{L}(p_\theta) = \lim_{\beta \to 0} \frac{\pd{H}{\theta}(\theta, p_\theta^\beta, \beta)-\pd{H}{\theta}(\theta, p_\theta^0, 0) }{\beta}
\end{equation}



\begin{equation}
 \frac{d}{d\theta}\frac{d}{d\beta}G(\theta,\beta)\big|_{\beta=0} = \frac{d}{d\theta} \mathcal{L}(p_\theta)
\end{equation}






2. Proof

3. EqProp with constraints



4. Proof of convergence

5. Error margins
\section{ Implementation}
0. Technical details
    architecture
    training
    etc
1. Stability techniques

2. algorithm
\section{Results}


\nocite{*}
\bibliographystyle{plain}
\bibliography{references.bib}



\section{Appendices}
\subsection{Proof of equilibrium propagation formula}
\end{document}

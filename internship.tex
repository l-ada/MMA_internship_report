\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}

% Title Page
\title{Title}
\author{≈Åukasz Adamowicz}

\usepackage{mathcommands}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}



\section{ Lab presentation}
SciAi groups research focus is on ...

 \section{Math part}
Notation: Let $E(\theta, M ,p)$ denote the energy model, with $\theta$ being model parameters, $M$ the molecule and $p\in \R^n$ density coefficients.

 \subsection{Implicit}
1. Implicit funciton theorem

The goal of the project was to investigate and minimize the following loss based on an approach in \cite{neuralscf}.
\begin{equation}
 \min_\theta \mathcal{L}(p_{\theta}),
\end{equation}
where
\begin{equation}
  p_\theta:=\argmin_p E(\theta,p)
\end{equation}
and $\mathcal{L}(p) = \frac{1}{2}\mse{p-p_{data}}$ is the standard $L_2$ loss.
This falls under the domain of \textbf{bilevel optimization}.

The main problem lies in computing the gradient of $\mathcal{L}(p_\theta)$


\subsubsection{ IFT with constraints}
Let $w$ be a non-zero vector.
We consider the constrained problem of minimizing

\begin{equation}
 \mathcal{L}(p_\theta),
\end{equation}
where
\begin{equation}
 p_\theta = \argmin_{p: \inner{p}{w}=N} E(\theta, p, w)
\end{equation}



3. Lagrangian approach?

TK COME BACK WHEN YOU ACTUALLY DO IT BY HAND
 \begin{equation}
\min_{\lambda,\theta} L
 \end{equation}

4. Proof of IFT with constraints converging

5. Error analysis for both approximate fixed point and solving lineare equation
\subsection{ Equilibrium propagation}

\subsubsection{Base version}

Let's denote
\begin{equation}
H(\theta, p, \beta) := \beta \mathcal{L}(p) + E(\theta, p),
\end{equation}
where $\beta \in \R$.
 This is called \textbf{total energy} in \cite{eqprop}. Let's also denote
\begin{equation}
p_{\theta}^{\beta}:=\argmin_p H(\theta, p, \beta)
\end{equation}
and
\begin{equation}
 G(\theta, \beta) = H(\theta, p_\theta^\beta, \beta).
\end{equation}

Since $H(\theta,p,0) = E(\theta,p)$ we have $p_{\theta}^{0}=p_{\theta}$.

Statement:

\begin{equation}
 \frac{d}{d\theta} \mathcal{L}(p_\theta) = \lim_{\beta \to 0} \frac{\pd{H}{\theta}(\theta, p_\theta^\beta, \beta)-\pd{H}{\theta}(\theta, p_\theta^0, 0) }{\beta}
\end{equation}



\begin{equation}
 \frac{d}{d\theta}\frac{d}{d\beta}G(\theta,\beta)\big|_{\beta=0} = \frac{d}{d\theta} \mathcal{L}(p_\theta)
\end{equation}






2. Proof

3. EqProp with constraints



4. Proof of convergence

5. Error margins
\section{ Implementation}
0. Technical details
    architecture
    training
    etc
1. Stability techniques

2. algorithm
\section{Results}


\nocite{*}
\bibliographystyle{plain}
\bibliography{references.bib}



\section{Appendices}
\subsection{Proof of equilibrium propagation formula}
\end{document}
